{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating embeddings using FaceNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "\n",
    "# load resnet from pretrained model 20180402-114759-vggface2.pt\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing usage of model\n",
    "\n",
    "embedding_path = \"datasets/ffhq256_pp/train/embeddings/00000.npy\"\n",
    "image_path = \"datasets/ffhq256/00000.png\"\n",
    "\n",
    "emb = np.load(embedding_path)\n",
    "\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img = mtcnn(img)\n",
    "\n",
    "img = img.reshape(1, 3, 160, 160)\n",
    "emb2 = resnet(img).detach().numpy()\n",
    "\n",
    "emb2-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate and save embeddings for all images in the dataset\n",
    "import numpy as np\n",
    "dataset_path = 'datasets/ffhq256/'\n",
    "\n",
    "import os\n",
    "img_files = os.listdir(dataset_path)\n",
    "img_files.sort()\n",
    "\n",
    "start_index = 49053\n",
    "end_index = len(img_files)\n",
    "\n",
    "for i in range(start_index, end_index):\n",
    "    img_path = dataset_path + img_files[i]\n",
    "    img_copy_path = dataset_path + 'images/' + img_files[i]\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_cropped = mtcnn(img, save_path=img_copy_path)\n",
    "    if img_cropped is None:\n",
    "        print('No face detected in image: ', img_files[i])\n",
    "        continue\n",
    "\n",
    "    if img_cropped.shape[0] > 1:\n",
    "        # remove the other faces\n",
    "        for j in range(1, img_cropped.shape[0]):\n",
    "            os.remove(img_copy_path.split('.')[0] + '_' + str(j+1) + '.png')\n",
    "        \n",
    "        img_cropped = img_cropped[0].reshape(1, 3, 160, 160)\n",
    "\n",
    "\n",
    "    try:\n",
    "        img_embedding = resnet(img_cropped)\n",
    "    except:\n",
    "        print('Error in processing image: ', img_files[i])\n",
    "        # remove the image copy\n",
    "        os.remove(img_copy_path)\n",
    "        continue\n",
    "    \n",
    "    embedding = img_embedding.detach().numpy()\n",
    "\n",
    "    file_name = dataset_path + 'embeddings/' + img_files[i].split('.')[0] + '.npy'\n",
    "    np.save(file_name, embedding)\n",
    "\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print('Processed ', i, ' images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv datasets/ffhq256_new/embeddings datasets/ffhq256_new/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split intro train test\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset_path = 'datasets/ffhq256/'\n",
    "img_files = os.listdir(dataset_path + 'images/')\n",
    "emb_files = os.listdir(dataset_path + 'embeddings/')\n",
    "img_files.sort()\n",
    "emb_files.sort()\n",
    "\n",
    "train_path = dataset_path + 'train/'\n",
    "test_path = dataset_path + 'test/'\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets/ffhq256_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /datasets/ffhq256/train /datasets/ffhq256_new/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
